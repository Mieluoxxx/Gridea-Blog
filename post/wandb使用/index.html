<html lang="zh">
<head>
    <meta charset="utf-8" />
<meta name="description" content="" />
<meta name="viewport" content="width=device-width, initial-scale=1" />

<title>wandbç‚¼ä¸¹ä¼´ä¾£ | å’•å™œå’•å™œé­”æ³•ä½¿</title>

<link rel="shortcut icon" href="https://lab.moguw.top/favicon.ico?v=1711027564217">

<link href="https://fastly.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" rel="stylesheet">
<link rel="stylesheet" href="https://lab.moguw.top/styles/main.css">
<!-- DEMO JS -->
<!--<script src="media/scripts/index.js"></script>-->



    
<link rel="stylesheet" href="https://fastly.jsdelivr.net/gh/highlightjs/cdn-release@11.3.1/build/styles/base16/solarized-light.min.css">

<script src="https://fastly.jsdelivr.net/combine/gh/highlightjs/cdn-release@11.3.1/build/highlight.min.js,gh/highlightjs/cdn-release@11.3.1/build/languages/cpp.min.js,gh/highlightjs/cdn-release@11.3.1/build/languages/csharp.min.js,gh/highlightjs/cdn-release@11.3.1/build/languages/c.min.js,gh/highlightjs/cdn-release@11.3.1/build/languages/python.min.js,gh/highlightjs/cdn-release@11.3.1/build/languages/rust.min.js,gh/highlightjs/cdn-release@11.3.1/build/languages/xml.min.js,gh/highlightjs/cdn-release@11.3.1/build/languages/javascript.min.js,gh/highlightjs/cdn-release@11.3.1/build/languages/powershell.min.js,gh/highlightjs/cdn-release@11.3.1/build/languages/shell.min.js,gh/highlightjs/cdn-release@11.3.1/build/languages/sql.min.js,gh/highlightjs/cdn-release@11.3.1/build/languages/http.min.js,gh/highlightjs/cdn-release@11.3.1/build/languages/go.min.js,gh/highlightjs/cdn-release@11.3.1/build/languages/css.min.js"></script>

<!-- <script src="https://fastly.jsdelivr.net/gh/highlightjs/cdn-release@11.3.1/build/highlight.min.js"></script>
<script src="https://fastly.jsdelivr.net/gh/highlightjs/cdn-release@11.3.1/build/languages/cpp.min.js"></script>
<script src="https://fastly.jsdelivr.net/gh/highlightjs/cdn-release@11.3.1/build/languages/csharp.min.js"></script>
<script src="https://fastly.jsdelivr.net/gh/highlightjs/cdn-release@11.3.1/build/languages/c.min.js"></script>
<script src="https://fastly.jsdelivr.net/gh/highlightjs/cdn-release@11.3.1/build/languages/python.min.js"></script>
<script src="https://fastly.jsdelivr.net/gh/highlightjs/cdn-release@11.3.1/build/languages/rust.min.js"></script>
<script src="https://fastly.jsdelivr.net/gh/highlightjs/cdn-release@11.3.1/build/languages/xml.min.js"></script>
<script src="https://fastly.jsdelivr.net/gh/highlightjs/cdn-release@11.3.1/build/languages/javascript.min.js"></script>
<script src="https://fastly.jsdelivr.net/gh/highlightjs/cdn-release@11.3.1/build/languages/powershell.min.js"></script>
<script src="https://fastly.jsdelivr.net/gh/highlightjs/cdn-release@11.3.1/build/languages/shell.min.js"></script>
<script src="https://fastly.jsdelivr.net/gh/highlightjs/cdn-release@11.3.1/build/languages/sql.min.js"></script>
<script src="https://fastly.jsdelivr.net/gh/highlightjs/cdn-release@11.3.1/build/languages/http.min.js"></script>
<script src="https://fastly.jsdelivr.net/gh/highlightjs/cdn-release@11.3.1/build/languages/go.min.js"></script>
<script src="https://fastly.jsdelivr.net/gh/highlightjs/cdn-release@11.3.1/build/languages/css.min.js"></script>
 -->

    <link rel="stylesheet" href="https://fastly.jsdelivr.net/npm/viewerjs@1.10.2/dist/viewer.min.css" />
<script src="https://fastly.jsdelivr.net/npm/viewerjs@1.10.2/dist/viewer.min.js"></script>

    <link rel="stylesheet" href="https://fastly.jsdelivr.net/npm/katex@0.12.0/dist/katex.css">
    
</head>
<body>
<div class="main gt-bg-theme-color-first">
    <nav class="navbar navbar-expand-lg">
    <a class="navbar-brand" href="/">
        <img class="user-avatar" src="/images/avatar.png" alt="å¤´åƒ">
        <div class="site-name gt-c-content-color-first">
            å’•å™œå’•å™œé­”æ³•ä½¿
        </div>
    </a>
    <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent"
        aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation" id="changeNavbar">
        <i class="fas fa-bars gt-c-content-color-first" style="font-size: 18px"></i>
    </button>
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
        <div class="navbar-nav mr-auto" style="text-align: center">
            
            <div class="nav-item">
                
                <a href="/" class="menu gt-a-link">
                    é¦–é¡µ
                </a>
                
            </div>
            
            <div class="nav-item">
                
                <a href="/archives" class="menu gt-a-link">
                    å½’æ¡£
                </a>
                
            </div>
            
            <div class="nav-item">
                
                <a href="/tags" class="menu gt-a-link">
                    æ ‡ç­¾
                </a>
                
            </div>
            
            <div class="nav-item">
                
                <a href="https://lab.moguw.top/tag/nl6HTtVUL/" class="menu gt-a-link">
                    è€ƒç ”408
                </a>
                
            </div>
            
        </div>
        <div style="text-align: center">
            <form id="gridea-search-form" style="position: relative" data-update="1711027564217"
                action="/search/">
                <input class="search-input" autocomplete="off" spellcheck="false" name="q" placeholder="æœç´¢æ–‡ç« " />
                <i class="fas fa-search gt-c-content-color-first" style="position: absolute; top: 9px; left: 10px;"></i>
            </form>
        </div>
    </div>
</nav>
<script>
    /* ç§»åŠ¨ç«¯å¯¼èˆªæ å±•å¼€/æ”¶èµ·åˆ‡æ¢ */
    document.getElementById('changeNavbar').onclick = function () {
        var element = document.getElementById('navbarSupportedContent');
        if (element.style.display === 'none' || element.style.display === '') {
            element.style.display = 'block';
        } else {
            element.style.display = 'none';
        }
    }
</script>

    <div class="post-container">
        <div class="post-detail gt-bg-theme-color-second">
            <article class="gt-post-content">
                <h2 class="post-title">
                    wandbç‚¼ä¸¹ä¼´ä¾£
                </h2>
                <div class="post-info">
                    <time class="post-time gt-c-content-color-first">
                        Â· 2023-12-14 Â·
                    </time>
                    
                        <a href="https://lab.moguw.top/tag/mgicuOei_c/" class="post-tags">
                            # å®ç”¨å·¥å…·
                        </a>
                    
                </div>
                <div class="post-content">
                    <h2 id="ä¸ºä»€ä¹ˆé€‰æ‹©wandb">ä¸ºä»€ä¹ˆé€‰æ‹©wandb</h2>
<figure data-type="image" tabindex="1"><img src="http://pic.moguw.top/i/2023/12/14/657b01b2abc85.png" alt="1702560177775.png" loading="lazy"></figure>
<p>ä¸‹é¢æ˜¯wandbçš„é‡è¦çš„å·¥å…·ï¼š</p>
<ul>
<li>Dashboardï¼šè·Ÿè¸ªå®éªŒï¼Œå¯è§†åŒ–ç»“æœï¼›</li>
<li>Reportsï¼šåˆ†äº«ï¼Œä¿å­˜ç»“æœï¼›</li>
<li>Sweepsï¼šè¶…å‚è°ƒä¼˜ï¼›</li>
<li>Artifactsï¼šæ•°æ®é›†å’Œæ¨¡å‹çš„ç‰ˆæœ¬æ§åˆ¶ã€‚</li>
</ul>
<h2 id="è·Ÿè¸ªå®éªŒ">è·Ÿè¸ªå®éªŒ</h2>
<h3 id="pytorch-mnist">Pytorch MNIST</h3>
<h4 id="å¯¼åŒ…">å¯¼åŒ…</h4>
<pre><code class="language-python">import os
import numpy as np
from torch.utils.data import DataLoader, Dataset
import torch
from torch import nn
import torchvision
from torchvision import transforms
import datetime
import wandb
from argparse import Namespace

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
</code></pre>
<h4 id="wanda-config">Wanda config</h4>
<pre><code class="language-python">config = Namespace(
    project_name='wandb_demo',

    batch_size=512,

    hidden_layer_width=64,
    dropout_p=0.1,

    lr=1e-4,
    optim_type='Adam',

    epochs=15,
    ckpt_path='checkpoint.pt'
)
</code></pre>
<h4 id="æ•°æ®åŠ è½½å™¨">æ•°æ®åŠ è½½å™¨</h4>
<pre><code class="language-python">def create_dataloaders(config):
    transform = transforms.Compose([transforms.ToTensor()])
    ds_train = torchvision.datasets.MNIST(root=&quot;./mnist/&quot;,train=True,download=True,transform=transform)
    ds_val = torchvision.datasets.MNIST(root=&quot;./mnist/&quot;,train=False,download=True,transform=transform)

    ds_train_sub = torch.utils.data.Subset(ds_train, indices=range(0, len(ds_train), 5))
    dl_train =  torch.utils.data.DataLoader(ds_train_sub, batch_size=config.batch_size, shuffle=True,
                                            num_workers=2,drop_last=True)
    dl_val =  torch.utils.data.DataLoader(ds_val, batch_size=config.batch_size, shuffle=False, 
                                          num_workers=2,drop_last=True)
    return dl_train,dl_val
</code></pre>
<h4 id="æ„å»ºæ¨¡å‹">æ„å»ºæ¨¡å‹</h4>
<pre><code class="language-python">class CustomNet(nn.Module):
    def __init__(self, config):
        super(CustomNet, self).__init__()
        self.conv1 = nn.Conv2d(in_channels=1, out_channels=config.hidden_layer_width, kernel_size=3)
        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)
        self.conv2 = nn.Conv2d(in_channels=config.hidden_layer_width,
                               out_channels=config.hidden_layer_width, kernel_size=5)
        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)
        self.dropout = nn.Dropout2d(p=config.dropout_p)
        self.adaptive_pool = nn.AdaptiveMaxPool2d((1, 1))
        self.flatten = nn.Flatten()
        self.linear1 = nn.Linear(config.hidden_layer_width, config.hidden_layer_width)
        self.relu = nn.ReLU()
        self.linear2 = nn.Linear(config.hidden_layer_width, 10)

    def forward(self, x):
        x = self.conv1(x)
        x = self.pool1(x)
        x = self.conv2(x)
        x = self.pool2(x)
        x = self.dropout(x)
        x = self.adaptive_pool(x)
        x = self.flatten(x)
        x = self.linear1(x)
        x = self.relu(x)
        x = self.linear2(x)
        return x
</code></pre>
<h4 id="è®­ç»ƒå‡½æ•°">è®­ç»ƒå‡½æ•°</h4>
<pre><code class="language-python">def train_epoch(model, dl_train, optimizer):
    model.train()
    for step, batch in enumerate(dl_train):
        features, labels = batch
        features, labels = features.to(device), labels.to(device)

        preds = model(features)
        loss = nn.CrossEntropyLoss()(preds, labels)
        loss.backward()

        optimizer.step()
        optimizer.zero_grad()
    return model
</code></pre>
<h4 id="æµ‹è¯•å‡½æ•°">æµ‹è¯•å‡½æ•°</h4>
<pre><code class="language-python">def eval_epoch(model, dl_val):
    model.eval()
    accurate = 0
    num_elems = 0
    for batch in dl_val:
        features, labels = batch
        features, labels = features.to(device), labels.to(device)
        with torch.no_grad():
            preds = model(features)
        predictions = preds.argmax(dim=-1)
        accurate_preds = (predictions == labels)
        num_elems += accurate_preds.shape[0]
        accurate += accurate_preds.long().sum()

    val_acc = accurate.item() / num_elems
    return val_acc
</code></pre>
<pre><code class="language-python">def train(config=config):
    dl_train, dl_val = create_dataloaders(config)
    model = CustomNet(config).to(device)
    optimizer = torch.optim.__dict__[config.optim_type](params=model.parameters(), lr=config.lr)
    # ======================================================================
    nowtime = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')
    wandb.init(project=config.project_name, config=config.__dict__, name=nowtime, save_code=True)
    model.run_id = wandb.run.id
    # ======================================================================
    model.best_metric = -1.0
    for epoch in range(1, config.epochs + 1):
        model = train_epoch(model, dl_train, optimizer)
        val_acc = eval_epoch(model, dl_val)
        if val_acc &gt; model.best_metric:
            model.best_metric = val_acc
            torch.save(model.state_dict(), config.ckpt_path)
        nowtime = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        print(f&quot;epochã€{epoch}ã€‘@{nowtime} --&gt; val_acc= {100 * val_acc:.2f}%&quot;)
        # ======================================================================
        wandb.log({'epoch': epoch, 'val_acc': val_acc, 'best_val_acc': model.best_metric})
        # ======================================================================
    # ======================================================================
    wandb.finish()
    # ======================================================================
    return model

model = train(config)
</code></pre>
<h2 id="ç‰ˆæœ¬æ§åˆ¶">ç‰ˆæœ¬æ§åˆ¶</h2>
<pre><code class="language-python">#resume the run 
import wandb 

run = wandb.init(project='wandb_demo', id= model.run_id, resume='must')
</code></pre>
<pre><code class="language-python"># save dataset 
arti_dataset = wandb.Artifact('mnist', type='dataset')
arti_dataset.add_dir('mnist/')
wandb.log_artifact(arti_dataset)
</code></pre>
<pre><code class="language-python"># save code 

arti_code = wandb.Artifact('ipynb', type='code')
arti_code.add_file('./mnist.ipynb')
wandb.log_artifact(arti_code)
</code></pre>
<pre><code class="language-python"># save model

arti_model = wandb.Artifact('cnn', type='model')
arti_model.add_file(config.ckpt_path)
wandb.log_artifact(arti_model)
</code></pre>
<pre><code class="language-python">wandb.finish() #finishæ—¶ä¼šæäº¤ä¿å­˜
</code></pre>
<h2 id="caseåˆ†æ">Caseåˆ†æ</h2>
<pre><code class="language-python">#resume the run 
import wandb 
run = wandb.init(project=config.project_name, id= model.run_id, resume='must')
</code></pre>
<pre><code class="language-python">import matplotlib.pyplot as plt 

transform = transforms.Compose([transforms.ToTensor()])
ds_train = torchvision.datasets.MNIST(root=&quot;./mnist/&quot;,train=True,download=True,transform=transform)
ds_val = torchvision.datasets.MNIST(root=&quot;./mnist/&quot;,train=False,download=True,transform=transform)
    
# visual the  prediction
device = None
for p in model.parameters():
    device = p.device
    break

plt.figure(figsize=(8,8)) 
for i in range(9):
    img,label = ds_val[i]
    tensor = img.to(device)
    y_pred = torch.argmax(model(tensor[None,...])) 
    img = img.permute(1,2,0)
    ax=plt.subplot(3,3,i+1)
    ax.imshow(img.numpy())
    ax.set_title(&quot;y_pred = %d&quot;%y_pred)
    ax.set_xticks([])
    ax.set_yticks([]) 
plt.show()    
</code></pre>
<pre><code class="language-python">def data2fig(data):
    import matplotlib.pyplot as plt 
    fig = plt.figure()
    ax = fig.add_subplot()
    ax.imshow(data)
    ax.set_xticks([])
    ax.set_yticks([]) 
    return fig

def fig2img(fig):
    import io,PIL
    buf = io.BytesIO()
    fig.savefig(buf)
    buf.seek(0)
    img = PIL.Image.open(buf)
    return img
</code></pre>
<pre><code class="language-python">from tqdm import tqdm 
good_cases = wandb.Table(columns = ['Image','GroundTruth','Prediction'])
bad_cases = wandb.Table(columns = ['Image','GroundTruth','Prediction'])
</code></pre>
<pre><code class="language-python"># æ‰¾åˆ°50ä¸ªgood cases å’Œ 50 ä¸ªbad cases

plt.close()

for i in tqdm(range(1000)):
    features,label = ds_val[i]
    tensor = features.to(device)
    y_pred = torch.argmax(model(tensor[None,...])) 
    
    # log badcase
    if y_pred!=label:
        if len(bad_cases.data)&lt;50:
            data = features.permute(1,2,0).numpy()
            input_img = wandb.Image(fig2img(data2fig(data)))
            bad_cases.add_data(input_img,label,y_pred)
            
    # log goodcase
    else:
        if len(good_cases.data)&lt;50:
            data = features.permute(1,2,0).numpy()
            input_img = wandb.Image(fig2img(data2fig(data)))
            good_cases.add_data(input_img,label,y_pred)
</code></pre>
<pre><code class="language-python">wandb.log({'good_cases':good_cases,'bad_cases':bad_cases})
</code></pre>
<pre><code class="language-python">wandb.finish()
</code></pre>
<h2 id="sweepå¯è§†åŒ–è‡ªåŠ¨è°ƒå‚">Sweepå¯è§†åŒ–è‡ªåŠ¨è°ƒå‚</h2>
<h3 id="é…ç½®-sweep-config">é…ç½® Sweep config</h3>
<p>é€‰æ‹©ä¸€ä¸ªè°ƒä¼˜ç®—æ³•</p>
<p>Sweepæ”¯æŒå¦‚ä¸‹3ç§è°ƒä¼˜ç®—æ³•:</p>
<p>(1)ç½‘æ ¼æœç´¢ï¼šgrid. éå†æ‰€æœ‰å¯èƒ½å¾—è¶…å‚ç»„åˆï¼Œåªåœ¨è¶…å‚ç©ºé—´ä¸å¤§çš„æ—¶å€™ä½¿ç”¨ï¼Œå¦åˆ™ä¼šéå¸¸æ…¢ã€‚</p>
<p>(2)éšæœºæœç´¢ï¼šrandom. æ¯ä¸ªè¶…å‚æ•°éƒ½é€‰æ‹©ä¸€ä¸ªéšæœºå€¼ï¼Œéå¸¸æœ‰æ•ˆï¼Œä¸€èˆ¬æƒ…å†µä¸‹å»ºè®®ä½¿ç”¨ã€‚</p>
<p>(3)è´å¶æ–¯æœç´¢ï¼šbayes. åˆ›å»ºä¸€ä¸ªæ¦‚ç‡æ¨¡å‹ä¼°è®¡ä¸åŒè¶…å‚æ•°ç»„åˆçš„æ•ˆæœï¼Œé‡‡æ ·æœ‰æ›´é«˜æ¦‚ç‡æå‡ä¼˜åŒ–ç›®æ ‡çš„è¶…å‚æ•°ç»„åˆã€‚å¯¹è¿ç»­å‹çš„è¶…å‚æ•°ç‰¹åˆ«æœ‰æ•ˆï¼Œä½†æ‰©å±•åˆ°éå¸¸é«˜ç»´åº¦çš„è¶…å‚æ•°æ—¶æ•ˆæœä¸å¥½ã€‚</p>
<pre><code class="language-python">sweep_config = {
    'method': 'random'  # grid bayes
    }
</code></pre>
<h3 id="å®šä¹‰è°ƒä¼˜ç›®æ ‡">å®šä¹‰è°ƒä¼˜ç›®æ ‡</h3>
<p>è®¾ç½®ä¼˜åŒ–æŒ‡æ ‡ï¼Œä»¥åŠä¼˜åŒ–æ–¹å‘ã€‚</p>
<p>sweep agents é€šè¿‡ wandb.log çš„å½¢å¼å‘ sweep controller ä¼ é€’ä¼˜åŒ–ç›®æ ‡çš„å€¼ã€‚</p>
<pre><code class="language-python">metric = {
    'name': 'val_acc',
    'goal': 'maximize'   
    }
sweep_config['metric'] = metric
</code></pre>
<h3 id="å®šä¹‰è¶…å‚ç©ºé—´">å®šä¹‰è¶…å‚ç©ºé—´</h3>
<p>è¶…å‚ç©ºé—´å¯ä»¥åˆ†æˆ å›ºå®šå‹ï¼Œç¦»æ•£å‹å’Œè¿ç»­å‹ã€‚</p>
<ul>
<li>å›ºå®šå‹ï¼šæŒ‡å®š value</li>
<li>ç¦»æ•£å‹ï¼šæŒ‡å®š valuesï¼Œåˆ—å‡ºå…¨éƒ¨å€™é€‰å–å€¼ã€‚</li>
<li>è¿ç»­æ€§ï¼šéœ€è¦æŒ‡å®š åˆ†å¸ƒç±»å‹ distribution, å’ŒèŒƒå›´ min, maxã€‚ç”¨äº random æˆ–è€… bayesé‡‡æ ·ã€‚</li>
</ul>
<pre><code class="language-python">sweep_config['parameters'] = {}

# å›ºå®šä¸å˜çš„è¶…å‚
sweep_config['parameters'].update({
    'project_name':{'value':'wandb_demo'},
    'epochs': {'value': 10},
    'ckpt_path': {'value':'checkpoint.pt'}})

# ç¦»æ•£å‹åˆ†å¸ƒè¶…å‚
sweep_config['parameters'].update({
    'optim_type': {
        'values': ['Adam', 'SGD','AdamW']
        },
    'hidden_layer_width': {
        'values': [16,32,48,64,80,96,112,128]
        }
    })

# è¿ç»­å‹åˆ†å¸ƒè¶…å‚
sweep_config['parameters'].update({
    
    'lr': {
        'distribution': 'log_uniform_values',
        'min': 1e-6,
        'max': 0.1
      },
    
    'batch_size': {
        'distribution': 'q_uniform',
        'q': 8,
        'min': 32,
        'max': 256,
      },
    
    'dropout_p': {
        'distribution': 'uniform',
        'min': 0,
        'max': 0.6,
      }
})
</code></pre>
<h3 id="å®šä¹‰å‰ªæç­–ç•¥-å¯é€‰">å®šä¹‰å‰ªæç­–ç•¥ (å¯é€‰)</h3>
<pre><code class="language-python">sweep_config['early_terminate'] = {
    'type':'hyperband',
    'min_iter':3,
    'eta':2,
    's':3
} #åœ¨step=3, 6, 12 æ—¶è€ƒè™‘æ˜¯å¦å‰ªæ
</code></pre>
<pre><code class="language-python">from pprint import pprint
pprint(sweep_config)
</code></pre>
<h3 id="åˆå§‹åŒ–-sweep-controller">åˆå§‹åŒ– sweep controller</h3>
<pre><code class="language-python">sweep_id = wandb.sweep(sweep_config, project=config.project_name)
</code></pre>
<h3 id="å¯åŠ¨-sweep-agent">å¯åŠ¨ Sweep agent</h3>
<pre><code class="language-python"># å‰ç½®ç•¥
# è¯¥agent éšæœºæœç´¢ å°è¯•5æ¬¡
wandb.agent(sweep_id, train, count=5)
</code></pre>
<h2 id="è°ƒå‚å¯è§†åŒ–å’Œè·Ÿè¸ª">è°ƒå‚å¯è§†åŒ–å’Œè·Ÿè¸ª</h2>
<h3 id="å¹³è¡Œåæ ‡ç³»å›¾">å¹³è¡Œåæ ‡ç³»å›¾</h3>
<figure data-type="image" tabindex="2"><img src="http://pic.moguw.top/i/2023/12/15/657b46ab702f3.png" alt="W&amp;B Chart 2023_12_15 02 16 14.png" loading="lazy"></figure>
<h3 id="è¶…å‚æ•°é‡è¦æ€§å›¾">è¶…å‚æ•°é‡è¦æ€§å›¾</h3>
<figure data-type="image" tabindex="3"><img src="http://pic.moguw.top/i/2023/12/15/657b46aa203a1.png" alt="1702577819364.png" loading="lazy"></figure>
<h2 id="å…¶ä»–æ¡ˆä¾‹">å…¶ä»–æ¡ˆä¾‹</h2>
<h3 id="pytorch-mnist2">PyTorch MNIST2</h3>
<h4 id="å¯¼åŒ…-2">å¯¼åŒ…</h4>
<pre><code class="language-python">import wandb
import math
import random
import torch, torchvision
import torch.nn as nn
import torchvision.transforms as T
from tqdm.notebook import tqdm


device = &quot;cuda:0&quot; if torch.cuda.is_available() else &quot;cpu&quot;
</code></pre>
<h4 id="æ•°æ®åŠ è½½å™¨-2">æ•°æ®åŠ è½½å™¨</h4>
<pre><code class="language-python">def get_dataloader(is_train, batch_size, slice=5, num_workers=2):
    &quot;&quot;&quot;
    Get a training or testing dataloader for the MNIST dataset.
    
    &quot;&quot;&quot;
    dataset = torchvision.datasets.MNIST(
        root=&quot;.&quot;,
        train=is_train,
        transform=T.ToTensor(),
        download=True
    )
    
    if is_train:
        shuffle = True
    else:
        shuffle = False

    subset_indices = range(0, len(dataset), slice)
    subset_dataset = torch.utils.data.Subset(dataset, subset_indices)

    loader = torch.utils.data.DataLoader(
        dataset=subset_dataset,
        batch_size=batch_size,
        shuffle=shuffle,
        pin_memory=True,
        num_workers=num_workers
    )

    return loader
</code></pre>
<h4 id="æ¨¡å‹æ­å»º">æ¨¡å‹æ­å»º</h4>
<pre><code class="language-python">class SimpleModel(nn.Module):
    def __init__(self, input_size=28*28, hidden_size=256, output_size=10, dropout=0.0):
        super(SimpleModel, self).__init__()
        self.flatten = nn.Flatten()
        self.fc1 = nn.Linear(input_size, hidden_size)
        self.bn1 = nn.BatchNorm1d(hidden_size)
        self.relu = nn.ReLU()
        self.dropout = nn.Dropout(dropout)
        self.fc2 = nn.Linear(hidden_size, output_size)

    def forward(self, x):
        x = self.flatten(x)
        x = self.fc1(x)
        x = self.bn1(x)
        x = self.relu(x)
        x = self.dropout(x)
        x = self.fc2(x)
        return x

def get_model(dropout=0.0):
    &quot;&quot;&quot;
    Create a simple neural network model.

    &quot;&quot;&quot;
    model = SimpleModel(dropout=dropout).to(device)
    return model
</code></pre>
<h4 id="éªŒè¯æ¨¡å‹">éªŒè¯æ¨¡å‹</h4>
<pre><code class="language-python"># è¯„ä¼°æ¨¡å‹åœ¨éªŒè¯æ•°æ®é›†ä¸Šçš„æ€§èƒ½
def validate_model(model, valid_dl, loss_func, log_images=False, batch_idx=0):
    &quot;Compute performance of the model on the validation dataset and log a wandb.Table&quot;
    model.eval()    # å°†æ¨¡å‹åˆ‡æ¢åˆ°è¯„ä¼°æ¨¡å¼ï¼Œè¿™ä¼šå½±å“åˆ°ä¸€äº›å…·æœ‰ä¸åŒè¡Œä¸ºçš„å±‚ï¼Œå¦‚ BatchNorm å’Œ Dropout
    val_loss = 0.
    with torch.inference_mode():    # è¿›å…¥æ¨æ–­æ¨¡å¼ï¼Œè¿™æ„å‘³ç€æ¨¡å‹å°†ä¸ä¼šè¿›è¡Œæ¢¯åº¦è®¡ç®—ï¼Œè¿™å¯ä»¥æé«˜å‰å‘ä¼ æ’­çš„æ•ˆç‡
        correct = 0
        for i, (images, labels) in tqdm(enumerate(valid_dl), leave=False):
            images, labels = images.to(device), labels.to(device)
            # Forward pass
            outputs = model(images)
            val_loss += loss_func(outputs, labels)*labels.size(0)
            # Compute accuracy and accumulate
            _, predicted = torch.max(outputs.data, 1)
            correct += (predicted == labels).sum().item()
            # Log one batch of images to the dashboard, always same batch_idx.
            if i==batch_idx and log_images: # é™ä½æ•°æ®é‡å’Œå‡å°‘æ—¥å¿—æ–‡ä»¶å¤§å°
                log_image_table(images, predicted, labels, outputs.softmax(dim=1))
    return val_loss / len(valid_dl.dataset), correct / len(valid_dl.dataset)
</code></pre>
<h4 id="wandbè®°å½•">wandbè®°å½•</h4>
<pre><code class="language-python">def log_image_table(images, predicted, labels, probs):
    &quot;Log a wandb.Table with (img, pred, target, scores)&quot;
    # ğŸ Create a wandb Table to log images, labels and predictions to
    table = wandb.Table(columns=[&quot;image&quot;, # è¡¨ç¤ºå›¾åƒåˆ—ï¼Œç”¨äºå­˜å‚¨å›¾åƒæ•°æ®
                                 &quot;pred&quot;, # è¡¨ç¤ºé¢„æµ‹åˆ—ï¼Œç”¨äºå­˜å‚¨æ¨¡å‹çš„é¢„æµ‹
                                 &quot;target&quot;] # è¡¨ç¤ºç›®æ ‡åˆ—ï¼Œç”¨äºå­˜å‚¨å®é™…æ ‡ç­¾
                        +[f&quot;score_{i}&quot; for i in range(10)]) # è¡¨ç¤ºåŒ…å«ç±»åˆ«åˆ†æ•°çš„åˆ—
    for img, pred, targ, prob in zip(images.to(&quot;cpu&quot;), predicted.to(&quot;cpu&quot;), labels.to(&quot;cpu&quot;), probs.to(&quot;cpu&quot;)):
        table.add_data(wandb.Image(img[0].numpy()*255), pred, targ, *prob.numpy())
    wandb.log({&quot;predictions_table&quot;:table}, commit=False)    # ä¸ç«‹å³æäº¤æ—¥å¿—ï¼Œå…è®¸åœ¨è®°å½•å¤šä¸ªæ—¥å¿—åä¸€æ¬¡æ€§æäº¤ï¼Œä»¥æé«˜æ•ˆç‡
</code></pre>
<h4 id="è®­ç»ƒ">è®­ç»ƒ</h4>
<pre><code class="language-python"># Launch 5 experiments, trying different dropout rates

for i in range(5):  # å¯åŠ¨äº”ä¸ªä¸åŒçš„å®éªŒï¼Œæ¯ä¸ªå®éªŒå…·æœ‰ä¸åŒçš„ dropout ç‡
    # ğŸ initialise a wandb run
    wandb.init(
        project=&quot;demo&quot;,
        name=&quot;pytorch_example&quot;+str(i),
        config={
            &quot;epochs&quot;: 10,
            &quot;batch_size&quot;: 128,
            &quot;lr&quot;: 1e-3,
            &quot;dropout&quot;: random.uniform(0.01, 0.80)})

    # Copy your config
    config = wandb.config

    # Get the data
    train_dl = get_dataloader(is_train=True, batch_size=config.batch_size)
    valid_dl = get_dataloader(is_train=False, batch_size=2*config.batch_size)
    n_steps_per_epoch = math.ceil(len(train_dl.dataset) / config.batch_size)

    # A simple MLP model
    model = get_model(config.dropout)

    # Make the loss and optimizer
    loss_func = nn.CrossEntropyLoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=config.lr)

   # Training
    example_ct = 0
    step_ct = 0
    for epoch in tqdm(range(config.epochs)):
        model.train()
        for step, (images, labels) in enumerate(tqdm(train_dl, leave=False)):
            images, labels = images.to(device), labels.to(device)
            outputs = model(images)
            train_loss = loss_func(outputs, labels)
            optimizer.zero_grad()
            train_loss.backward()
            optimizer.step()

            example_ct += len(images)
            metrics = {&quot;train/train_loss&quot;: train_loss,
                       &quot;train/epoch&quot;: (step + 1 + (n_steps_per_epoch * epoch)) / n_steps_per_epoch,
                       &quot;train/example_ct&quot;: example_ct}

            if step + 1 &lt; n_steps_per_epoch:
                # ğŸ Log train metrics to wandb
                wandb.log(metrics)
            step_ct += 1

        val_loss, accuracy = validate_model(model, valid_dl, loss_func, log_images=(epoch==(config.epochs-1)))

        # ğŸ Log train and validation metrics to wandb
        val_metrics = {&quot;val/val_loss&quot;: val_loss,
                       &quot;val/val_accuracy&quot;: accuracy}
        wandb.log({**metrics, **val_metrics})
        print(f&quot;Train Loss: {train_loss:.3f}, Valid Loss: {val_loss:3f}, Accuracy: {accuracy:.2f}&quot;)

    # If you had a test set, this is how you could log it as a Summary metric
    wandb.summary['test_accuracy'] = 0.8

    # ğŸ Close your wandb run
    wandb.finish()
</code></pre>
<h3 id="æ€»ç»“å­¦ä¹ çš„ç‚¹">æ€»ç»“å­¦ä¹ çš„ç‚¹</h3>
<h4 id="wandaè®°å½•çš„æ•°æ®">wandaè®°å½•çš„æ•°æ®</h4>
<pre><code class="language-python">def log_image_table(images, predicted, labels, probs):
    &quot;Log a wandb.Table with (img, pred, target, scores)&quot;
    # ğŸ Create a wandb Table to log images, labels and predictions to
    table = wandb.Table(columns=[&quot;image&quot;, # è¡¨ç¤ºå›¾åƒåˆ—ï¼Œç”¨äºå­˜å‚¨å›¾åƒæ•°æ®
                                 &quot;pred&quot;, # è¡¨ç¤ºé¢„æµ‹åˆ—ï¼Œç”¨äºå­˜å‚¨æ¨¡å‹çš„é¢„æµ‹
                                 &quot;target&quot;] # è¡¨ç¤ºç›®æ ‡åˆ—ï¼Œç”¨äºå­˜å‚¨å®é™…æ ‡ç­¾
                        +[f&quot;score_{i}&quot; for i in range(10)]) # è¡¨ç¤ºåŒ…å«ç±»åˆ«åˆ†æ•°çš„åˆ—
    for img, pred, targ, prob in zip(images.to(&quot;cpu&quot;), predicted.to(&quot;cpu&quot;), labels.to(&quot;cpu&quot;), probs.to(&quot;cpu&quot;)):
        table.add_data(wandb.Image(img[0].numpy()*255), pred, targ, *prob.numpy())
    wandb.log({&quot;predictions_table&quot;:table}, commit=False)    # ä¸ç«‹å³æäº¤æ—¥å¿—ï¼Œå…è®¸åœ¨è®°å½•å¤šä¸ªæ—¥å¿—åä¸€æ¬¡æ€§æäº¤ï¼Œä»¥æé«˜æ•ˆç‡
</code></pre>
<h4 id="éªŒè¯å‡½æ•°çš„æ„å»º">éªŒè¯å‡½æ•°çš„æ„å»º</h4>
<pre><code class="language-python"># è¯„ä¼°æ¨¡å‹åœ¨éªŒè¯æ•°æ®é›†ä¸Šçš„æ€§èƒ½
def validate_model(model, valid_dl, loss_func, log_images=False, batch_idx=0):
    &quot;Compute performance of the model on the validation dataset and log a wandb.Table&quot;
    model.eval()    # å°†æ¨¡å‹åˆ‡æ¢åˆ°è¯„ä¼°æ¨¡å¼ï¼Œè¿™ä¼šå½±å“åˆ°ä¸€äº›å…·æœ‰ä¸åŒè¡Œä¸ºçš„å±‚ï¼Œå¦‚ BatchNorm å’Œ Dropout
    val_loss = 0.
    with torch.inference_mode():    # è¿›å…¥æ¨æ–­æ¨¡å¼ï¼Œè¿™æ„å‘³ç€æ¨¡å‹å°†ä¸ä¼šè¿›è¡Œæ¢¯åº¦è®¡ç®—ï¼Œè¿™å¯ä»¥æé«˜å‰å‘ä¼ æ’­çš„æ•ˆç‡
        correct = 0
        for i, (images, labels) in tqdm(enumerate(valid_dl), leave=False):
            images, labels = images.to(device), labels.to(device)
            # Forward pass
            outputs = model(images)
            val_loss += loss_func(outputs, labels)*labels.size(0)
            # Compute accuracy and accumulate
            _, predicted = torch.max(outputs.data, 1)
            correct += (predicted == labels).sum().item()
            # Log one batch of images to the dashboard, always same batch_idx.
            if i==batch_idx and log_images: # é™ä½æ•°æ®é‡å’Œå‡å°‘æ—¥å¿—æ–‡ä»¶å¤§å°
                log_image_table(images, predicted, labels, outputs.softmax(dim=1))
    return val_loss / len(valid_dl.dataset), correct / len(valid_dl.dataset)
</code></pre>
<h2 id="pytorch-cifar10">Pytorch CIFAR10</h2>
<h3 id="å¯¼åŒ…-3">å¯¼åŒ…</h3>
<pre><code class="language-python">from __future__ import print_function
import argparse
import random # to set the python random seed
import numpy # to set the numpy random seed
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torchvision import datasets, transforms
# Ignore excessive warnings
import logging
logging.propagate = False 
logging.getLogger().setLevel(logging.ERROR)

# WandB â€“ Import the wandb library
import wandb
</code></pre>
<h3 id="æ„å»ºæ¨¡å‹-2">æ„å»ºæ¨¡å‹</h3>
<pre><code class="language-python">class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        
        # In our constructor, we define our neural network architecture that we'll use in the forward pass.
        # Conv2d() adds a convolution layer that generates 2 dimensional feature maps to learn different aspects of our image
        self.conv1 = nn.Conv2d(3, 6, kernel_size=5)
        self.conv2 = nn.Conv2d(6, 16, kernel_size=5)
        
        # Linear(x,y) creates dense, fully connected layers with x inputs and y outputs
        # Linear layers simply output the dot product of our inputs and weights.
        self.fc1 = nn.Linear(16 * 5 * 5, 120)
        self.fc2 = nn.Linear(120, 84)
        self.fc3 = nn.Linear(84, 10)

    def forward(self, x):
        # Here we feed the feature maps from the convolutional layers into a max_pool2d layer.
        # The max_pool2d layer reduces the size of the image representation our convolutional layers learnt,
        # and in doing so it reduces the number of parameters and computations the network needs to perform.
        # Finally we apply the relu activation function which gives us max(0, max_pool2d_output)
        x = F.relu(F.max_pool2d(self.conv1(x), 2))
        x = F.relu(F.max_pool2d(self.conv2(x), 2))
        
        # Reshapes x into size (-1, 16 * 5 * 5) so we can feed the convolution layer outputs into our fully connected layer
        x = x.view(-1, 16 * 5 * 5)
        
        # We apply the relu activation function and dropout to the output of our fully connected layers
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        x = self.fc3(x)
        
        # Finally we apply the softmax function to squash the probabilities of each class (0-9) and ensure they add to 1.
        return F.log_softmax(x, dim=1)
</code></pre>
<pre><code class="language-python">def train(config, model, device, train_loader, optimizer, epoch):
# Switch model to training mode. This is necessary for layers like dropout, batchnorm etc which behave differently in training and evaluation mode
    model.train()
    
    # We loop over the data iterator, and feed the inputs to the network and adjust the weights.
    for batch_idx, (data, target) in enumerate(train_loader):
        if batch_idx &gt; 20:
          break
        # Load the input features and labels from the training dataset
        data, target = data.to(device), target.to(device)
        
        # Reset the gradients to 0 for all learnable weight parameters
        optimizer.zero_grad()
        
        # Forward pass: Pass image data from training dataset, make predictions about class image belongs to (0-9 in this case)
        output = model(data)
        
        # Define our loss function, and compute the loss
        loss = F.nll_loss(output, target)
        
        # Backward pass: compute the gradients of the loss w.r.t. the model's parameters
        loss.backward()
        
        # Update the neural network weights
        optimizer.step()
</code></pre>
<pre><code class="language-python">def test(args, model, device, test_loader, classes):
    # Switch model to evaluation mode. This is necessary for layers like dropout, batchnorm etc which behave differently in training and evaluation mode
    model.eval()
    test_loss = 0
    correct = 0

    example_images = []
    with torch.no_grad():
        for data, target in test_loader:
            # Load the input features and labels from the test dataset
            data, target = data.to(device), target.to(device)
            
            # Make predictions: Pass image data from test dataset, make predictions about class image belongs to (0-9 in this case)
            output = model(data)
            
            # Compute the loss sum up batch loss
            test_loss += F.nll_loss(output, target, reduction='sum').item()
            
            # Get the index of the max log-probability
            pred = output.max(1, keepdim=True)[1]
            correct += pred.eq(target.view_as(pred)).sum().item()
            
            # WandB â€“ Log images in your test dataset automatically, along with predicted and true labels by passing pytorch tensors with image data into wandb.Image
            example_images.append(wandb.Image(
                data[0], caption=&quot;Pred: {} Truth: {}&quot;.format(classes[pred[0].item()], classes[target[0]])))
    
    # WandB â€“ wandb.log(a_dict) logs the keys and values of the dictionary passed in and associates the values with a step.
    # You can log anything by passing it to wandb.log, including histograms, custom matplotlib objects, images, video, text, tables, html, pointclouds and other 3D objects.
    # Here we use it to log test accuracy, loss and some test images (along with their true and predicted labels).
    wandb.log({
        &quot;Examples&quot;: example_images,
        &quot;Test Accuracy&quot;: 100. * correct / len(test_loader.dataset),
        &quot;Test Loss&quot;: test_loss})
</code></pre>
<h3 id="wandbé…ç½®">wandbé…ç½®</h3>
<pre><code class="language-python"># WandB â€“ Initialize a new run
wandb.init(project=&quot;demo&quot;)
wandb.watch_called = False # Re-run the model without restarting the runtime, unnecessary after our next release

# WandB â€“ Config is a variable that holds and saves hyperparameters and inputs
config = wandb.config          # Initialize config
config.batch_size = 4          # input batch size for training (default: 64)
config.test_batch_size = 10    # input batch size for testing (default: 1000)
config.epochs = 50             # number of epochs to train (default: 10)
config.lr = 0.1               # learning rate (default: 0.01)
config.momentum = 0.1          # SGD momentum (default: 0.5) 
config.no_cuda = False         # disables CUDA training
config.seed = 42               # random seed (default: 42)
config.log_interval = 10     # how many batches to wait before logging training status
</code></pre>
<h3 id="ä¸»å‡½æ•°">ä¸»å‡½æ•°</h3>
<pre><code class="language-python">def main():
    use_cuda = not config.no_cuda and torch.cuda.is_available()
    device = torch.device(&quot;cuda&quot; if use_cuda else &quot;cpu&quot;)
    kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}
    
    # Set random seeds and deterministic pytorch for reproducibility
    # random.seed(config.seed)       # python random seed
    torch.manual_seed(config.seed) # pytorch random seed
    # numpy.random.seed(config.seed) # numpy random seed
    torch.backends.cudnn.deterministic = True

    # Load the dataset: We're training our CNN on CIFAR10 (https://www.cs.toronto.edu/~kriz/cifar.html)
    # First we define the tranformations to apply to our images
    transform = transforms.Compose(
    [transforms.ToTensor(),
     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])
    
    # Now we load our training and test datasets and apply the transformations defined above
    train_loader = torch.utils.data.DataLoader(datasets.CIFAR10(root='./data', train=True,
                                              download=True, transform=transform), batch_size=config.batch_size,
                                              shuffle=True, **kwargs)
    test_loader = torch.utils.data.DataLoader(datasets.CIFAR10(root='./data', train=False,
                                             download=True, transform=transform), batch_size=config.test_batch_size,
                                             shuffle=False, **kwargs)

    classes = ('plane', 'car', 'bird', 'cat',
               'deer', 'dog', 'frog', 'horse', 'ship', 'truck')

    # Initialize our model, recursively go over all modules and convert their parameters and buffers to CUDA tensors (if device is set to cuda)
    model = Net().to(device)
    optimizer = optim.SGD(model.parameters(), lr=config.lr,
                          momentum=config.momentum)
    
    # WandB â€“ wandb.watch() automatically fetches all layer dimensions, gradients, model parameters and logs them automatically to your dashboard.
    # Using log=&quot;all&quot; log histograms of parameter values in addition to gradients
    wandb.watch(model, log=&quot;all&quot;)

    for epoch in range(1, config.epochs + 1):
        train(config, model, device, train_loader, optimizer, epoch)
        test(config, model, device, test_loader, classes)
        
    # WandB â€“ Save the model checkpoint. This automatically saves a file to the cloud and associates it with the current run.
    torch.save(model.state_dict(), &quot;model.h5&quot;)
    wandb.save('model.h5')

if __name__ == '__main__':
    main()
</code></pre>

                </div>
            </article>
        </div>

        
            <div class="next-post">
                <div class="next gt-c-content-color-first">ä¸‹ä¸€ç¯‡</div>
                <a href="https://lab.moguw.top/post/æœ€å°ç”Ÿæˆæ ‘/" class="post-title gt-a-link">
                    æœ€å°ç”Ÿæˆæ ‘
                </a>
            </div>
        

        

        

        

        <div class="site-footer gt-c-content-color-first">
    <div class="slogan gt-c-content-color-first">çœŸçœŸå¤œå¤œçš„å°æœ¨å±‹</div>
    <div class="social-container">
        
            
                <a href="https://github.com/Mieluoxxx" target="_blank">
                    <i class="fab fa-github gt-c-content-color-first"></i>
                </a>
            
        
            
        
            
        
            
        
            
        
            
        
            
        
    </div>
    <div class="footer-info">
        Powered by <a href="https://github.com/getgridea/gridea" target="_blank">Gridea</a>
    </div>
    <div>
        Theme <a href="https://github.com/imhanjie/gridea-theme-pure" target="_blank">Pure</a>, Powered by <a
                href="https://gridea.dev" target="_blank">Gridea</a> | <a href="https://lab.moguw.top/atom.xml" target="_blank">RSS</a>
    </div>
</div>

        <script>
  hljs.highlightAll();
</script>

        <script>
(function() {
	function addEventListener(element, type, handler) {
        if (element.addEventListener) {
            element.addEventListener(type, handler, false);
        } else if (element.attachEvent) {
            element.attachEvent('on' + type, handler);
        }
    }

    addEventListener(window, 'load', function () {
        var Viewer = window.Viewer;
        var pictures = document.querySelector('.post-content');
        var viewer = new Viewer(pictures);
    });
})();
</script>


        <script>
            var ele = document.querySelector(".waline-visitor-count");
            Object.defineProperty(ele, 'innerText', {
                get: function() {
                    return ele.innerText;
                },
                set: function(value) {
                    ele.innerHTML = value;
                    if (value > 0) {
	                    var parent = document.querySelector(".leancloud_visitors");
	                    parent.style.visibility="visible";
	                }
                }
            });
        </script>
    </div>
</div>
</body>
</html>
